exp_dir: exps/
mixed_precision: 'fp16'
allow_tf32: False
seed: 114514
ckpt_type: 'safetensors' # [torch, safetensors]

vis_info:
  prompt: null
  negative_prompt: ''

logger:
  -
    _target_: hcpdiff.loggers.CLILogger
    _partial_: True
    out_path: 'train.log'
    log_step: 20

text_encoder: null

plugin_unet: null
plugin_TE: null

unet: null

lora_unet:
  - lr: 1e-4
    rank: 0.01875
    branch: p
    dropout: 0.1
    layers:
      - 're:.*\.to_k$'
      - 're:.*\.to_v$'
      - 're:.*\.ff$'
      #- 're:.*\.attn.?$' # Increases fitness, but potentially reduces controllability
  - lr: 4e-5 # Low negative unet lr prevents image collapse
    rank: 0.01875
    branch: n
    dropout: 0.1
    layers:
      - 're:.*\.to_k$'
      - 're:.*\.to_v$'
      - 're:.*\.ff$'
      #- 're:.*\.attn.?$' # Increases fitness, but potentially reduces controllability
  #  - lr: 1e-4
  #    rank: 0.01875
  #    type: p
  #    layers:
  #      - 're:.*\.resnets$' # Increases fitness, but potentially reduces controllability and change style
  #  - lr: 4e-5
  #    rank: 0.01875
  #    type: n
  #    layers:
  #      - 're:.*\.resnets$' # Increases fitness, but potentially reduces controllability and change style

lora_text_encoder:
  - lr: 2e-5
    rank: 2
    branch: p
    dropout: 0.1
    layers:
      - 're:.*self_attn$'
      - 're:.*mlp$'
  - lr: 2e-5
    rank: 2
    branch: n
    dropout: 0.1
    layers:
      - 're:.*self_attn$'
      - 're:.*mlp$'

tokenizer_pt:
  train: # prompt tuning embeddings
    - { name: 'pt-botdog1', lr: 0.0025 }
    - { name: 'pt-botdog1-neg', lr: 0.0025 }

train:
  gradient_accumulation_steps: 1
  save_step: 100

  #cfg_scale: '1.0-3.0:cos' # dynamic CFG with timestamp
  cfg_scale: '3.0'

  loss:
    criterion: # min SNR loss
      _target_: hcpdiff.loss.MinSNRLoss
      gamma: 2.0

  scheduler:
    name: one_cycle
    num_warmup_steps: 200
    num_training_steps: 1000
    scheduler_kwargs: { }

  scheduler_pt:
    name: one_cycle
    num_warmup_steps: 200
    num_training_steps: 1000
    scheduler_kwargs: {}

model:
  pretrained_model_name_or_path: 'runwayml/stable-diffusion-v1-5'
  tokenizer_repeats: 1
  ema_unet: 0
  ema_text_encoder: 0
  clip_skip: 0

data:
  dataset1:
    batch_size: 4
    cache_latents: True
    loss_weight: 1.0

    source:
      data_source1:
        img_root: 'imgs/'
        prompt_template: 'prompt_tuning_template/object.txt'
        caption_file: null # path to image captions (file_words)
        tag_transforms:
          transforms:
            - _target_: hcpdiff.utils.caption_tools.TemplateFill
              word_names:
                pt1: [pt-botdog1, pt-botdog1-neg]
    bucket:
      _target_: hcpdiff.data.bucket.RatioBucket.from_files # aspect ratio bucket
      target_area: {_target_: "builtins.eval", _args_: ['512*512']}
      num_bucket: 1


  # Add regularization to prevent image crashes
  # Regularization images is generated by model itself with prompt from dataset
  dataset_class:
    _target_: hcpdiff.data.TextImagePairDataset
    _partial_: True
    batch_size: 1
    cache_latents: True
    att_mask_encode: False
    loss_weight: 1.0

    source:
      data_source1:
        img_root: 'imgs/v15'
        prompt_template: 'prompt_tuning_template/caption.txt'
        caption_file: 'imgs/v15/image_captions.json'
        att_mask: null
        bg_color: [255, 255, 255] # RGB; for ARGB -> RGB
        image_transforms:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
        tag_transforms:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: hcpdiff.utils.caption_tools.TagShuffle
            - _target_: hcpdiff.utils.caption_tools.TagDropout
              p: 0.1
            - _target_: hcpdiff.utils.caption_tools.TemplateFill
              word_names:
                pt1: ''
    bucket:
      _target_: hcpdiff.data.bucket.FixedBucket
      target_size: [512, 512]
