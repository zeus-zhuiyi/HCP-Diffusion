# _base_: [cfgs/infer/text2img.yaml]

# base_state*base_model_alpha + (lora_state[i]*lora_scale[i]*lora_alpha[i]) + (part_state[k]*part_alpha[k])

pretrained_model: ''
prompt: ''
neg_prompt: 'lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry'
out_dir: 'output/'
emb_dir: 'embs/'
N_repeats: 1
clip_skip: 0
bs: 4
num: 1
seed: null
dtype: 'fp16'

interface:
  - _target_: hcpdiff.vis.WebUIInterface
    show_steps: 1
    save_root: 'output/'

condition: null

save:
  save_cfg: True
  image_type: png
  quality: 95
#  image_type: webp
#  quality: 75

offload: null
#offload:
#  max_VRAM: 8GiB
#  max_RAM: 30GiB
#  vae_cpu: False

infer_args:
  width: 512
  height: 512
  guidance_scale: 7.5
  num_inference_steps: 50

new_components: {}

merge:
  group1:
    type: 'unet'
    base_model_alpha: 1.0 # base model weight to merge with lora or part
    lora:
      - path: ''
        alpha: 0.65
        layers: 'all'
        mask: [ 0.5, 1 ] #
      - path: ''
        alpha: 0.65
        layers: 'all'
        mask: [ 0, 0.5 ]
    part: null

  group2:
    type: 'TE'
    base_model_alpha: 1.0 # base model weight to infer with lora or part
    lora: null
    part: null